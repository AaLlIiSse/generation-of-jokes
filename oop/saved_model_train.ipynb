{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "DEVICE = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load(r'C:\\Users\\Alice\\Downloads\\python\\oop\\saved_model\\pytorch_model.bin',map_location=torch.device('cuda:0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(r'C:\\Users\\Alice\\Downloads\\python\\oop\\saved_model').to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(r'C:\\Users\\Alice\\Downloads\\python\\oop\\saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Подарить трусы с авакадо Владимиру Харченко?\n",
      "- Нет, Абрам Львович. Это Вам не трусы с авакадо.\n",
      "- Почему?\n",
      "- А такое было условие конкурса: написать рассказ о том, как один человек потерял свой автомобиль. \n"
     ]
    }
   ],
   "source": [
    "# Пример вероятностного сэмплирвоания с ограничением\n",
    "def generate_joke():\n",
    "    text = \"- Подарить трусы с авакадо Владимиру Харченко?\"\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(input_ids, \n",
    "                            do_sample=True,\n",
    "                            num_beams=2,\n",
    "                            temperature=1.5,\n",
    "                            top_p=0.9,\n",
    "                            max_length=70,\n",
    "                            )\n",
    "\n",
    "    generated_text = list(map(tokenizer.decode, out))[0]\n",
    "    extra_list = []\n",
    "\n",
    "    for line in range(0, len(generated_text)-3):\n",
    "        if (generated_text[line] == '[') and (generated_text[line + 1] == \"S\") and (generated_text[line + 2] == 'J') and (generated_text[line + 3] == ']'):\n",
    "            break\n",
    "        extra_list.append(generated_text[line])\n",
    "\n",
    "    final_joke = \"\".join(extra_list)\n",
    "    return final_joke[:-5:]\n",
    "\n",
    "    #print('------------------------------------')\n",
    "\n",
    "joke = generate_joke()\n",
    "print(joke)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 29 23:39:51 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.78       Driver Version: 512.78       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   47C    P8     6W /  N/A |      0MiB /  6144MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57ce36cfcf249843349f3236835148d06497510cfc71a5adee1576d1704ee7ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
